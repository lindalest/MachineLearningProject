{"name":"Machinelearningproject","tagline":"HTML of project for Machine Learning Coursera course","body":"---\r\ntitle: \"Machine Learning Project\"\r\nauthor: \"Linda Lester\"\r\ndate: \"August 4, 2015\"\r\noutput: html_document\r\n---\r\n## Executive Summary\r\nThe goal of this project is to predict the manner of the exercise, classe,  by building a model using one of several variable available in the data base.  \r\nwhat you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. \r\nStart with question- input data- features-algorithm-parameters-evaluation\r\ntraining set- pick features, use cross validation\r\nDoes the submission build a machine learning algorithm to predict activity quality from activity monitors?\r\n\r\n\r\n\r\n\r\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\r\n\r\nUnilateral Dumbbell Biceps Curl, Unilateral Dumbbell Triceps Exten- sion and Unilateral Dumbbell Lateral Raise.\r\nBecause of the characteristic noise in the sensor data, we used a Random Forest approach [28]. This algorithm is characterized by a subset of features, selected in a random and independent manner with the same distribution for each of the trees in the forest. To improve recognition performance we used an ensemble of classifiers using the “Bagging” method [6]. We used 10 random forests and each forest was implemented with 10 trees. The classifier was tested with 10-fold cross-validation and different windows sizes, all of them with 0.5s overlapping (except the window with 0.5s). The best window size found for this classification task was of 2.5s and the overall recognition performance was of 98.03% (see Table 1). \r\nThe confusion matrix of the leave-one-subject-out test is illustrated on Figure 2.\r\n\r\nUsing homogeonous data to predict the outcome.  I cleaned the data base by removing variables that were unlikely to improve the prediction of the exercise outcome.\r\nI removed columns containg identifying information and muliple NAs. My trianing models included 53 variables from roll belt to gyros forearm.\r\n```{r, echo=F}\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(tidyr)\r\nlibrary(lattice)\r\nlibrary(caret)\r\nlibrary(AppliedPredictiveModeling)\r\nlibrary(randomForest)\r\nlibrary(rpart)\r\nlibrary(party)\r\nlibrary(e1071)\r\nlibrary(ROCR)\r\nlibrary(rattle)\r\n\r\n\r\npml <- read.csv(\"pml-training.csv\", stringsAsFactors=F)\r\n\r\nView(pml)\r\n\r\n#split variables into homogeneous groups\r\nsub <- select(pml, c(roll_belt:total_accel_belt, gyros_belt_x:total_accel_arm, gyros_arm_x:magnet_arm_z, roll_dumbbell:yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x:yaw_forearm, total_accel_forearm, gyros_forearm_x:classe))\r\nnewsub <- mutate(sub, fcasse=as.factor(classe))\r\nnewsub$classe <- NULL\r\n```\r\nDeveloping a algort to predict exericse outcomes\r\nDefine the error rate for prediction and split the data into a training and testing subset. I split 80% of the data into the training group, leaving 20% for the training group. \r\nI choose the random forest method for training because it works with classification data as found in this data base and is capable of dealing with a high noise rate. Finally I used cross validation\r\n\r\n```{r}\r\nView(newsub)\r\nset.seed(7598)\r\ntrainpml = createDataPartition(newsub$fcasse, p =0.80, list=FALSE)\r\ntraining = newsub[trainpml,]\r\ntesting = newsub[-trainpml,]\r\nfit1a <- train( fcasse ~ ., method = \"rf\", data=training, trcontrol= trainControl(method = \"cv\", number =7), prox= TRUE, allowParallel=TRUE) \r\ntr <- getTree(fit1a$finalMode)\r\nfit1g <- rpart(fcasse ~ ., data=training, method=\"class\")\r\nprintRandomForests(fit1a, models=NULL)\r\nprint(fit1a$finalModel)\r\nErrorPlot <- plot(tr, main=\"Classification Tree\")\r\n\r\ntr <- getTree(fancyRpartPlot(fit1g, main =\"Example of Tree\"))\r\nconfmat <- confusionMatrix(fit1a, training, type = \"prob\")\r\n```\r\n\r\n\r\n\r\n## Estimating Error with Cross Validation\r\n\r\nThe estimated out of bag error for this model was 0.58% based upon the training set data. \r\nWhen the model was applied to the testing data set the calculated error rate was 0.008% better than the estimate OOB.\r\n## Confusion Matrix\r\n###A confusion matrix shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data. The matrix is NxN, where N is the number of target values (classes). Performance of such models is commonly evaluated using the data in the matrix. The following table displays a 2x2 confusion matrix for two classes (Positive and Negative).\r\nTesting the model to determine its abiity to accurately predict the classe of exercise. I applied my model to the tesing portion of the data.  \r\nThe accuracy rate was caculated at 99.23% using this model.\r\n\r\ncfplot <- ggplot(fit1a$finalModel)\r\ncfplot + geom_step(fill=Freq) + scale_x_discrete\r\n```{r}\r\ntesta <- predict(fit1a, testing)\r\ngetTree(testa)\r\nplot(testa) + testa +geom_map(fill=Freq)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n## Use model to predict class in new test data\r\n\r\n```{r, echo=FALSE}\r\npmltest <- read.csv(\"pml-testing.csv\", stringsAsFactors = FALSE)\r\nView(pmltest)\r\npredict(fit1a, pmltest)\r\n```\r\n\r\n\r\n# Submission to Coursera\r\n```{r}\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  path <- \"predictionAssignment_files/answers\"\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=file.path(path, filename),quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\npml_write_files(hat)\r\n```\r\n\r\nNote that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}